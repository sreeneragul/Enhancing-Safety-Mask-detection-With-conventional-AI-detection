
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9eKHX5DSXTZ"
      },
      "outputs": [],
      "source": [
        "# import module\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data  # Data.Dataset , Data.DataLoader\n",
        "import cv2\n",
        "from torchvision import transforms\n",
        "\n",
        "torch.manual_seed(1)  # Reproducible\n",
        "\n",
        "# constant\n",
        "imgSize = 64  # Images are [64 x 64]\n",
        "\n",
        "\n",
        "# Hyper-Parameters\n",
        "EPOCH = 300  # train the training data n times, to save time, we just train 1 epoch\n",
        "BATCH_SIZE = 50\n",
        "LR = 0.001  # learning rate\n",
        "kernel_size = 3\n",
        "padding = int((kernel_size-1)/2)\n",
        "filterNum = 10\n",
        "\n",
        "# Data Path\n",
        "TestPath = \"./TestFace\"\n",
        "TrainPath = \"./TrainFace\"\n",
        "\n",
        "\n",
        "# ----- Create Dataset ----- #\n",
        "class TorchDataset(Data.Dataset):\n",
        "    def __init__(self, filePath, repeat=1):\n",
        "        \"\"\"\n",
        "        :param filePath: the director where store the Images(test or train)\n",
        "        :param res_ImgSize: default = 64\n",
        "        :param repeat: the repeat times of all sample, default is 1 time\n",
        "        \"\"\"\n",
        "        self.filePath = filePath\n",
        "        self.image_label_list = self.read_file(filePath)\n",
        "        self.len = len(self.image_label_list)\n",
        "        self.repeat = repeat\n",
        "\n",
        "        '''class torchvision.transforms.ToTensor'''\n",
        "        self.toTensor = transforms.ToTensor()\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        index = i % self.len\n",
        "        label = np.array(self.image_label_list[index])\n",
        "        ImgContain = cv2.imread(str(self.filePath + '/' + str(index) + '.jpg'))\n",
        "        ImgContain = cv2.resize(ImgContain, (64, 64), interpolation=cv2.INTER_CUBIC)\n",
        "        # ImgContain.transpose(2, 0, 1)\n",
        "        Imgdata = ImgContain.transpose(2, 0, 1)\n",
        "        Imgdata = torch.from_numpy(np.asarray(Imgdata))\n",
        "        Imgdata = Imgdata.type(torch.FloatTensor) / 255.\n",
        "        return Imgdata, label\n",
        "\n",
        "    def __len__(self):\n",
        "        data_len = len(self.image_label_list) * self.repeat\n",
        "        return data_len\n",
        "\n",
        "    def read_file(self, filePath):\n",
        "        # load Data label\n",
        "        print(filePath + \"!!!\")\n",
        "        image_label = np.load(filePath + '/SubimgLabel.npy')\n",
        "        return image_label\n",
        "\n",
        "    def data_preproccess(self, data):\n",
        "        data = self.toTensor(data)\n",
        "        return data\n",
        "#-----------------------------#\n",
        "\n",
        "# load Train label and Test label\n",
        "Test_label = np.load(TestPath + '/SubimgLabel.npy')\n",
        "Train_label = np.load(TrainPath + '/SubimgLabel.npy')\n",
        "\n",
        "\n",
        "# Data Loader for easy mini-batch return in training, the image batch shape will be (50, 3, 64, 64) # 3 color -> (R,G,B)\n",
        "train_data = TorchDataset(filePath=TrainPath, repeat=1)\n",
        "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_data = TorchDataset(filePath=TestPath, repeat=1)\n",
        "test_loader = Data.DataLoader(dataset=test_data, batch_size=len(test_data), shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "# ----- Construct CNN ----- #\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28) [3 x 64 x 64]\n",
        "            nn.Conv2d(\n",
        "                in_channels=3,              # input height\n",
        "                out_channels=filterNum*3,             # filters:10, out_channel=3*3=9\n",
        "                kernel_size=kernel_size,              # filter size [9 x 9]\n",
        "                stride=1,                   # filter movement/step\n",
        "                padding=padding,                  # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
        "            ),                              # output shape [9 x 64 x 64]\n",
        "            #nn.Dropout(0.5),                # drop 50% of the neuron\n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape [9 x 32 x 32]\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(         # input shape [9 x 32 x 32]\n",
        "            nn.Conv2d(filterNum*3, filterNum*3*2, kernel_size, 1, padding),     # output shape [18 x 32 x 32]\n",
        "            #nn.Dropout(0.5),                # drop 50% of the neuron\n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool2d(2),                # output shape [18 x 16 x 16]\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(         # input shape  [18 x 16 x 16]\n",
        "            nn.Conv2d(filterNum*3*2, filterNum*3*2*2, kernel_size, 1, padding),     # output shape [36 x 16 x 16]\n",
        "            #nn.Dropout(0.5),                # drop 50% of the neuron\n",
        "            nn.ReLU(),                      # activation\n",
        "            nn.MaxPool2d(2),                # output shape  [36 x 8 x 8]\n",
        "        )\n",
        "        self.out = nn.Linear((filterNum*3*2*2) * 8 * 8, 3)   # fully connected layer, output 3 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        #with torch.no_grad():\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to [batch_size, 120 x 8 x 8]\n",
        "        output = self.out(x)\n",
        "        return output, x    # return x for visualization\n",
        "\n",
        "# ---------------------- #\n",
        "\n",
        "cnn = CNN()\n",
        "print(cnn)  # net architecture\n",
        "\n",
        "# optimize all cnn parameters\n",
        "optimizer = torch.optim.Adam(cnn.parameters(),lr=LR, betas=(0.9, 0.999), eps=1e-08,weight_decay=0.01)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "total_size = Train_label.sum()\n",
        "weights = total_size/sum(Train_label)\n",
        "class_weights = torch.FloatTensor(weights).to(device)\n",
        "loss_func = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "\n",
        "# training and testing\n",
        "TrainAcc = []\n",
        "TestAcc = []\n",
        "TrainLoss = []\n",
        "TestLoss = []\n",
        "TrainPredict = []\n",
        "isFirst = True\n",
        "\n",
        "cnn = cnn.to(device)\n",
        "for epoch in range(EPOCH):\n",
        "    train_acc = 0.\n",
        "    train_loss = 0.\n",
        "    i = 0\n",
        "    for step, (b_x, b_y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
        "        b_x = b_x.to(device)\n",
        "        b_y = b_y.to(device)\n",
        "        cnn.train()\n",
        "        output = cnn(b_x)[0]               # cnn output\n",
        "        B_y = torch.max(b_y, 1)[1]\n",
        "        loss = loss_func(output, B_y)   # cross entropy loss\n",
        "        train_loss += loss.data\n",
        "        optimizer.zero_grad()           # clear gradients for this training step\n",
        "        loss.backward()                 # backpropagation, compute gradients\n",
        "        optimizer.step()                # apply gradients\n",
        "\n",
        "        # training error rate\n",
        "        pred = torch.max(output, 1)[1]\n",
        "        num_correct = (pred == B_y).sum()\n",
        "        train_acc += float(num_correct.data)\n",
        "        if epoch == 2:\n",
        "            if isFirst == True:\n",
        "                TrainPredict = (output.float().cpu()).detach().numpy()\n",
        "                isFirst = False\n",
        "            else:\n",
        "                TrainPredict = np.append(TrainPredict,np.array((output.float().cpu()).detach()), axis=0)\n",
        "                #TrainPredict.append(np.array((output.float().cpu()).detach()))\n",
        "\n",
        "\n",
        "\n",
        "    cnn.eval()\n",
        "    eval_loss2 = 0.\n",
        "    eval_acc = 0.\n",
        "\n",
        "    for i, (t_x, t_y) in enumerate(test_loader):\n",
        "        t_x = t_x.to(device)\n",
        "        t_y = t_y.to(device)\n",
        "        output2 = cnn(t_x)[0]\n",
        "        T_y = torch.max(t_y, 1)[1]\n",
        "        loss2 = loss_func(output2, T_y)\n",
        "        eval_loss2 += loss2.data\n",
        "        #print(T_y)\n",
        "        pred2 = torch.max(output2, 1)[1]\n",
        "        #print(pred2)\n",
        "        num_correct2 = (pred2 == T_y).sum()\n",
        "        eval_acc += float(num_correct2.data)\n",
        "\n",
        "    test_rate = eval_acc / float(len(test_data))\n",
        "    train_rate = train_acc / float(len(train_data))\n",
        "    print(\"-----Epoch\"+str(epoch)+\"-----\")\n",
        "    print('Test Acc: {:.6f}'.format(test_rate))\n",
        "    print('Train Acc: {:.6f}'.format(train_rate))\n",
        "    # testloss: tensoe type -> cpu.float\n",
        "    TestAcc.append(test_rate)\n",
        "    TestLoss.append((eval_loss2.cpu().item()) / float(len(test_data)))\n",
        "    TrainAcc.append(train_rate)\n",
        "    TrainLoss.append((train_loss.cpu().item()) / float(len(train_data)))\n",
        "\n",
        "\n",
        "np.save(\"TrainPredict.npy\",np.array(TrainPredict))\n",
        "np.save(\"TestPredict.npy\",(output2.float().cpu()).detach().numpy())\n",
        "np.save(\"TestAcc.npy\",np.array(TestAcc))\n",
        "np.save(\"TestLoss.npy\",np.array(TestLoss))\n",
        "np.save(\"TrainAcc.npy\",np.array(TrainAcc))\n",
        "np.save(\"TrainLoss.npy\",np.array(TrainLoss))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import module\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import cv2\n",
        "# change directory\n",
        "\n",
        "os.chdir(r\"C:\\Users\\...\\CNN\")  # modify the current directory!!!\n",
        "ImagePath = \"./images/\"\n",
        "StorePath = \"./TrainFace\"\n",
        "# Cerate folder if it doesn't exist\n",
        "if not os.path.isdir(StorePath):\n",
        "    os.mkdir(StorePath)\n",
        "\n",
        "ImgSize = []\n",
        "ImgLabel = []\n",
        "index = -1\n",
        "ImageName = \"\"\n",
        "\n",
        "# Open CSV file\n",
        "with open('train.csv', newline='') as csvFile:\n",
        "\n",
        "    # read the content of CSV file, transform each row to a dictionary\n",
        "    rows = csv.DictReader(csvFile)\n",
        "\n",
        "    # resize each image to [64 x 64] pixels & export data for CNN classification\n",
        "\n",
        "    for row in rows:\n",
        "        print('Part ' + str(index))\n",
        "        print(row['filename'], row['width'], row['height'], row['xmin'],  row['ymin'], row['xmax'], row['ymax'], row['label'])\n",
        "        index = int(index + 1)\n",
        "        # store subImage size => [subwidth, subheight]\n",
        "        ImgSize.append([int(row['xmax']) - int(row['xmin']) + 1, int(row['ymax']) - int(row['ymin']) + 1])\n",
        "        if row['label'] == 'good':\n",
        "            ImgLabel.append([1, 0, 0])\n",
        "        elif row['label'] == 'bad':\n",
        "            ImgLabel.append([0, 1, 0])\n",
        "        else:\n",
        "            ImgLabel.append([0, 0, 1])\n",
        "        # load New image\n",
        "        if row['filename'] != ImageName:\n",
        "            ImageName = row['filename']\n",
        "            # e.g. : img = cv2.imread('./images/000_1OC3DT.jpg')\n",
        "            ImgContain = cv2.imread(ImagePath + ImageName)\n",
        "\n",
        "        subImgContain = ImgContain[int(row['ymin'])-1:int(row['ymax']), int(row['xmin'])-1:int(row['xmax'])]\n",
        "\n",
        "\n",
        "        SubimageName = StorePath + '/' + str(index) + '.jpg'\n",
        "        resizeImg = cv2.resize(subImgContain, (64, 64), interpolation=cv2.INTER_CUBIC)\n",
        "        cv2.imwrite(SubimageName, resizeImg)\n",
        "\n",
        "# automaticly close file\n",
        "\n",
        "np.save(StorePath + \"/SubimgSize.npy\", ImgSize)\n",
        "np.save(StorePath + \"/SubimgLabel.npy\",ImgLabel)"
      ],
      "metadata": {
        "id": "XAxEQKxVSlgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "sKC-3qJxSaZK"
      }
    }
  ]
}
